{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355UKMUQJxFd"
   },
   "source": [
    "# Scalable Diffusion Models with Transformer (DiT)\n",
    "\n",
    "This notebook samples from pre-trained DiT models. DiTs are class-conditional latent diffusion models trained on ImageNet that use transformers in place of U-Nets as the DDPM backbone. DiT outperforms all prior diffusion models on the ImageNet benchmarks.\n",
    "\n",
    "[Project Page](https://www.wpeebles.com/DiT) | [HuggingFace Space](https://huggingface.co/spaces/wpeebles/DiT) | [Paper](http://arxiv.org/abs/2212.09748) | [GitHub](github.com/facebookresearch/DiT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJlgLkSaKn7u"
   },
   "source": [
    "# 1. Setup\n",
    "\n",
    "We recommend using GPUs (Runtime > Change runtime type > Hardware accelerator > GPU). Run this cell to clone the DiT GitHub repo and setup PyTorch. You only have to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (0.19.1)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.1-cp38-cp38-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: sympy in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is MPS (Metal) available? True\n",
      "Is CUDA available? False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Is MPS (Metal) available?\", torch.backends.mps.is_available())\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DiT' already exists and is not an empty directory.\n",
      "Requirement already satisfied: diffusers in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (0.32.2)\n",
      "Requirement already satisfied: timm in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (1.0.15)\n",
      "Requirement already satisfied: importlib-metadata in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (8.5.0)\n",
      "Requirement already satisfied: filelock in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.2 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (0.29.3)\n",
      "Requirement already satisfied: numpy in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from diffusers) (10.2.0)\n",
      "Requirement already satisfied: torch in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from timm) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from timm) (0.19.1)\n",
      "Requirement already satisfied: pyyaml in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from huggingface-hub>=0.23.2->diffusers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from huggingface-hub>=0.23.2->diffusers) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.13.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from importlib-metadata->diffusers) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->diffusers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->diffusers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from requests->diffusers) (2023.11.17)\n",
      "Requirement already satisfied: sympy in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from sympy->torch->timm) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arunavsatyaraj/.pyenv/versions/3.8.18/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/DiT.git\n",
    "import DiT, os\n",
    "os.chdir('DiT')\n",
    "os.environ['PYTHONPATH'] = '/env/python:/content/DiT'\n",
    "!pip install diffusers timm --upgrade\n",
    "# DiT imports:\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from download import find_model\n",
    "from models import DiT_XL_2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "torch.set_grad_enabled(False)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  ##Since I am working on macbook I needed to change the GPU\n",
    "if device == \"cpu\":\n",
    "    print(\"GPU not found. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXpziRkoOvV9"
   },
   "source": [
    "# Download DiT-XL/2 Models\n",
    "\n",
    "You can choose between a 512x512 model and a 256x256 model. You can swap-out the LDM VAE, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EWG-WNimO59K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arunavsatyaraj/Desktop/DiT/DiT/download.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(local_path, map_location=lambda storage, loc: storage)\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Set image parameters\n",
    "image_size = 256\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\"\n",
    "latent_size = image_size // 8\n",
    "\n",
    "# Load model\n",
    "model = DiT_XL_2(input_size=latent_size).to(device)\n",
    "state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()  # Important for inference\n",
    "vae = AutoencoderKL.from_pretrained(vae_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JTNyzNZKb9E"
   },
   "source": [
    "# 2. Sample from Pre-trained DiT Models\n",
    "\n",
    "You can customize several sampling options. For the full list of ImageNet classes, [check out this](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-Hw7B5h4Kk4p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(y\u001b[38;5;241m=\u001b[39my, cfg_scale\u001b[38;5;241m=\u001b[39mcfg_scale)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Sample images\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Remove null class samples\u001b[39;00m\n\u001b[1;32m     31\u001b[0m samples \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mdecode(samples \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.18215\u001b[39m)\u001b[38;5;241m.\u001b[39msample\n",
      "File \u001b[0;32m~/Desktop/DiT/DiT/diffusion/gaussian_diffusion.py:450\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03mGenerate samples from the model.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m:param model: the model module.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m:return: a non-differentiable batch of samples.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m final \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_sample_loop_progressive(\n\u001b[1;32m    451\u001b[0m     model,\n\u001b[1;32m    452\u001b[0m     shape,\n\u001b[1;32m    453\u001b[0m     noise\u001b[38;5;241m=\u001b[39mnoise,\n\u001b[1;32m    454\u001b[0m     clip_denoised\u001b[38;5;241m=\u001b[39mclip_denoised,\n\u001b[1;32m    455\u001b[0m     denoised_fn\u001b[38;5;241m=\u001b[39mdenoised_fn,\n\u001b[1;32m    456\u001b[0m     cond_fn\u001b[38;5;241m=\u001b[39mcond_fn,\n\u001b[1;32m    457\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    458\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    459\u001b[0m     progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m    460\u001b[0m ):\n\u001b[1;32m    461\u001b[0m     final \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/DiT/DiT/diffusion/gaussian_diffusion.py:501\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop_progressive\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress)\u001b[0m\n\u001b[1;32m    499\u001b[0m t \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor([i] \u001b[38;5;241m*\u001b[39m shape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 501\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m out\n\u001b[1;32m    511\u001b[0m     img \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/DiT/DiT/diffusion/gaussian_diffusion.py:402\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_sample\u001b[39m(\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    378\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    385\u001b[0m ):\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    Sample x_{t-1} from the model at the given timestep.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    :param model: the model to sample from.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m             - 'pred_xstart': a prediction of x_0.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     noise \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn_like(x)\n\u001b[1;32m    411\u001b[0m     nonzero_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    412\u001b[0m         (t \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m([\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    413\u001b[0m     )  \u001b[38;5;66;03m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DiT/DiT/diffusion/respace.py:92\u001b[0m, in \u001b[0;36mSpacedDiffusion.p_mean_variance\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mp_mean_variance\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=signature-differs\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DiT/DiT/diffusion/gaussian_diffusion.py:288\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model_output\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B, C \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    287\u001b[0m model_output, model_var_values \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msplit(model_output, C, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 288\u001b[0m min_log \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_into_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_log_variance_clipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m max_log \u001b[38;5;241m=\u001b[39m _extract_into_tensor(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbetas), t, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# The model_var_values is [-1, 1] for [min_var, max_var].\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DiT/DiT/diffusion/gaussian_diffusion.py:870\u001b[0m, in \u001b[0;36m_extract_into_tensor\u001b[0;34m(arr, timesteps, broadcast_shape)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_into_tensor\u001b[39m(arr, timesteps, broadcast_shape):\n\u001b[1;32m    862\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;124;03m    Extract values from a 1-D numpy array for a batch of indices.\u001b[39;00m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m    :param arr: the 1-D numpy array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 870\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[timesteps]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(broadcast_shape):\n\u001b[1;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# Set user inputs\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250\n",
    "cfg_scale = 4\n",
    "class_labels = torch.tensor([207, 360, 387, 974, 88, 979, 417, 279], dtype=torch.int64, device=device)\n",
    "samples_per_row = 4\n",
    "\n",
    "# Create diffusion object\n",
    "diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# Create sampling noise\n",
    "n = len(class_labels)\n",
    "z = torch.randn(n, 4, latent_size, latent_size, dtype=torch.float32, device=device)\n",
    "y = class_labels.to(device)  # Keep labels as int64 for embeddings\n",
    "\n",
    "# Setup classifier-free guidance\n",
    "z = torch.cat([z, z], 0)\n",
    "y_null = torch.full((n,), 1000, dtype=torch.int64, device=device)  # Null class labels\n",
    "y = torch.cat([y, y_null], 0)\n",
    "\n",
    "# Model input dictionary\n",
    "model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# Sample images\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model.forward_with_cfg, z.shape, z, clip_denoised=False, \n",
    "    model_kwargs=model_kwargs, progress=True, device=device\n",
    ")\n",
    "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# Save and display images\n",
    "save_image(samples, \"sample.png\", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))\n",
    "samples = Image.open(\"sample.png\")\n",
    "display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
